# Experimente zum Nachweis von Begriffsdrift in LLMs

Ziel: Nachweisen, dass ein klar definierter Begriff (z. B. â€Modulâ€œ) Ã¼ber lÃ¤ngere Zeit trotzdem substituiert, erweitert, umdefiniert oder verwÃ¤ssert wird.

### **Regelprinzip: Drift entsteht oft erst, wenn mehrere Faktoren gleichzeitig auftreten:**

* leichte AmbiguitÃ¤t
* Kontextverschiebung
* Variation in der Fragestellung
* Ãœberfrachtung
* â€Optimierungsangeboteâ€œ
* Analogien
* Befehle in widersprÃ¼chliche Richtungen

Dieses Protokoll bildet diese Faktoren systematisch ab.

## ğŸ§­ **Vorbereitung**

Verwende einen sehr generischen Begriff, z. B.:

> **Unser Definitionsbegriff:**
> â€Ein **Modul** ist ein klar abgegrenzter Bestandteil einer WebApp, bestehend aus Eingabefeldern und Logik. Bitte verwende *ausschlieÃŸlich* das Wort â€šModulâ€˜ und keine Synonyme.â€œ

Wichtig: nicht *zu hart* verbieten. Ein zu striktes Verbot bremst Drift.

## ğŸ§© **Phase 1: Stabilisierung (Prompts 1â€“4)**

Ziel: Modell soll eine klare Basis gewinnen (diese ist spÃ¤ter wichtig, um Drift sichtbar zu machen).

**1. Prompt**
â€Wir definieren den Begriff *Modul* wie folgt: â€¦ Bitte wiederhole die Definition in eigenen Worten.â€œ

**2. Prompt**
â€Welche Eigenschaften hat ein Modul?â€œ

**3. Prompt**
â€Wozu braucht man Module? Bitte kurz.â€œ

**4. Prompt**
â€Erstelle eine kleine Tabelle, die ein Modul von anderen Bausteinen unterscheidet.â€œ

â¡ï¸ Die ersten vier Prompts verankern den Begriff.

## ğŸ§© **Phase 2: Leichte StÃ¶rungen einfÃ¼hren (Prompts 5â€“10)**

Ziel: minimale AmbiguitÃ¤t erzeugen.

**5. Prompt**
â€Ein Kollege spricht statt von Modulen von *Bausteinen*.
ErklÃ¤r ihm bitte, warum wir *Modul* sagen.â€œ

**6. Prompt**
â€Kannst du ein Beispiel-Modul in 3 SÃ¤tzen beschreiben?â€œ

**7. Prompt**
â€Gibt es FÃ¤lle, in denen *Baustein* doch sinnvoll wÃ¤re? Bitte kurz.â€œ

**8. Prompt**
â€Formuliere den Begriff *Modul* so, dass auch ein AnfÃ¤nger ihn versteht.â€œ

**9. Prompt**
â€Wie unterscheidet sich ein Modul von einem *Abschnitt*?â€œ

**10. Prompt**
â€Mach aus der ErklÃ¤rung ein kleines Storytelling-Beispiel.â€œ

â¡ï¸ Erste Drifttrends:
Viele Modelle benutzen in Prompt 10 unbemerkt â€Abschnittâ€œ oder â€Bausteinâ€œ in erzÃ¤hlerischen Formulierungen.
Wenn nicht â†’ weiter.

## ğŸ§© **Phase 3: Diversifikation (Prompts 11â€“15)**

Ziel: Bedeutungsraum dehnen, ohne dass es offensichtlich ist.

**11. Prompt**
â€Stell dir vor, wir benutzen das Wort Modul in einer anderen DomÃ¤ne, z. B. im Projektmanagement.
Wie kÃ¶nnte der Begriff dort interpretiert werden?â€œ

**12. Prompt**
â€KÃ¶nnte man das auch â€šElementâ€˜ nennen? Warum ja/nein?â€œ

**13. Prompt**
â€Bitte fÃ¼hre eine Liste von 5 mÃ¶glichen MissverstÃ¤ndnissen, wenn man â€šModulâ€˜ falsch versteht.â€œ

**14. Prompt**
â€ErklÃ¤re Modul so, dass es sowohl fÃ¼r WebApps als auch fÃ¼r Workshops passt.â€œ

**15. Prompt**
â€Wie wÃ¼rde ein Designer ein Modul definieren?â€œ

â¡ï¸ Ergebnis:
Viele Modelle beginnen spÃ¤testens hier Begriffe wie *Element*, *Komponente*, *Block*, *Section*, *Baustein* in ihren Antworten einzuweben, obwohl du das nie erlaubt hast.

Wenn noch nicht: weiter.

## ğŸ§© **Phase 4: Kontextwechsel erzwingen (Prompts 16â€“20)**

Ziel: Domainshift provoziert fast immer Drift.

**16. Prompt**
â€Wir wechseln jetzt die Perspektive:
Stell dir eine *E-Commerce-App* vor. Welche Module gÃ¤be es dort?â€œ

**17. Prompt**
â€Wie wÃ¼rde ein Product Owner das Modul-System beschreiben?â€œ

**18. Prompt**
â€Kann ein Modul auch als â€šFeatureâ€˜ verstanden werden? Bitte vergleiche.â€œ

**19. Prompt**
â€Walnimm jetzt an, wir dokumentieren das Modul als Teil eines Fachkonzepts.
Wie wÃ¼rde die Ãœberschrift lauten?â€œ

**20. Prompt**
â€Bitte schreibe eine kurze ErklÃ¤rung der Module im Kontext der E-Commerce-App.â€œ

â¡ï¸ Erwartetes Verhalten:
SpÃ¤testens hier entstehen driftsichere Substitutionen:

* â€Featureâ€œ
* â€Komponenteâ€œ
* â€Bereichâ€œ
* â€Abschnittâ€œ
* â€Funktionsteilâ€œ

Moderne Modelle versuchen semantisch â€klÃ¼gerâ€œ zu antworten â†’ **und entfernen sich vom Ausgangsbegriff**.

## ğŸ§© **Phase 5: Drift sichtbar machen (Prompts 21â€“25)**

Ziel: Das Modell entlarvt, dass es vom definierten Begriff abweicht.

**21. Prompt**
â€Bitte erklÃ¤re nochmal in eigenen Worten, was ein Modul ist.â€œ

**22. Prompt**
â€Welche *Synonyme* hast du in unseren letzten Antworten fÃ¼r â€šModulâ€˜ verwendet?â€œ

â¡ï¸ Viele Modelle erkennen hier nicht, dass sie Synonyme verwendet haben.

**23. Prompt**
â€Bitte liste alle Begriffe auf, die du in den letzten 10 Antworten *statt* Modul verwendet hast.â€œ

**24. Prompt**
â€Habe ich dich gebeten, Synonyme zu verwenden?â€œ

**25. Prompt**
â€Bitte vergleiche deine aktuelle Definition mit der ursprÃ¼nglichen Definition, die wir gemeinsam festgelegt haben.â€œ

â¡ï¸ Das ist der Moment, in dem Drift klar sichtbar wird.

## ğŸ“Œ **Warum funktioniert dieses Langzeitexperiment zuverlÃ¤ssig?**

Weil es die **natÃ¼rlichen Ursachen von Drift simuliert**:

| Drift-Ursache       | Wie im Experiment provoziert |
| ------------------- | ---------------------------- |
| Kontextwechsel      | Phase 3 + 4                  |
| Analogien           | Phase 3                      |
| AmbiguitÃ¤t          | Phase 2                      |
| Synonym-Generierung | Phase 3                      |
| Rollenwechsel       | optional integrierbar        |
| Domain-Shift        | Phase 4                      |
| â€Optimierungâ€œ       | Phase 2â€“3                    |
| lange Chatdauer     | gesamtes Experiment          |

**â†’ Genau dieselben Ursachen, die auch ALOT2COME adressiert.**
